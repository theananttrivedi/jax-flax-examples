{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Self Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nnx.Module):\n",
    "    def __init__(self, d_model: int = 2, \n",
    "                 row_dim: int = 0, \n",
    "                 col_dim: int = 1, \n",
    "                 *, \n",
    "                 rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W_q = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.W_k = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.W_v = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def __call__(self, token_encodings: jax.Array, mask: jax.Array = None):\n",
    "        q = token_encodings @ self.W_q\n",
    "        k = token_encodings @ self.W_k\n",
    "        v = token_encodings @ self.W_v\n",
    "\n",
    "        sims = q @ k.swapaxes(self.row_dim, self.col_dim)\n",
    "        scaled_sims = sims / jnp.sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Mask out values with a large negative number\n",
    "            scaled_sims = jnp.where(mask, -1e9, scaled_sims)\n",
    "\n",
    "        attention_percents = jax.nn.softmax(scaled_sims, axis=self.col_dim)\n",
    "        attention_scores = attention_percents @ v\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Dry Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "[[False  True  True]\n",
      " [False False  True]\n",
      " [False False False]]\n",
      "\n",
      "Masked Self-Attention Output:\n",
      "[[0.8224545  0.52411664]\n",
      " [1.3705109  0.99325544]\n",
      " [0.95412695 0.5512294 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create input and model\n",
    "encodings_matrix = jnp.array([[1.16, 0.23],\n",
    "                             [0.57, 1.36],\n",
    "                             [4.41, -2.16]])\n",
    "\n",
    "# Set random seed\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Create masked self-attention object\n",
    "masked_self_attention = MaskedSelfAttention(d_model=2,\n",
    "                                          row_dim=0,\n",
    "                                          col_dim=1,\n",
    "                                          rngs=nnx.Rngs(params=key))\n",
    "\n",
    "# Create causal mask (lower triangular)\n",
    "mask = jnp.tril(jnp.ones((3, 3))) == 0\n",
    "print(\"Mask:\")\n",
    "print(mask)\n",
    "\n",
    "# Calculate masked self-attention\n",
    "output = masked_self_attention(encodings_matrix, mask)\n",
    "print(\"\\nMasked Self-Attention Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query weights (W_q) transposed:\n",
      "[[0.5302608  0.90153027]\n",
      " [0.31336212 0.6983329 ]]\n",
      "\n",
      "Key weights (W_k) transposed:\n",
      "[[0.5302608  0.90153027]\n",
      " [0.31336212 0.6983329 ]]\n",
      "\n",
      "Value weights (W_v) transposed:\n",
      "[[0.5302608  0.90153027]\n",
      " [0.31336212 0.6983329 ]]\n",
      "\n",
      "Queries (q):\n",
      "[[ 0.8224545   0.52411664]\n",
      " [ 1.5283298   1.1283492 ]\n",
      " [ 0.3911445  -0.12647225]]\n",
      "\n",
      "Keys (k):\n",
      "[[ 0.8224545   0.52411664]\n",
      " [ 1.5283298   1.1283492 ]\n",
      " [ 0.3911445  -0.12647225]]\n",
      "\n",
      "Values (v):\n",
      "[[ 0.8224545   0.52411664]\n",
      " [ 1.5283298   1.1283492 ]\n",
      " [ 0.3911445  -0.12647225]]\n",
      "\n",
      "Similarity scores (sims):\n",
      "[[0.9511297  1.8483683  0.25541237]\n",
      " [1.8483683  3.608964   0.45509294]\n",
      " [0.25541237 0.45509294 0.16898927]]\n",
      "\n",
      "Scaled similarities:\n",
      "[[0.67255026 1.3069937  0.18060382]\n",
      " [1.3069937  2.551923   0.3217993 ]\n",
      " [0.18060382 0.3217993  0.11949346]]\n",
      "\n",
      "Masked scaled similarities:\n",
      "[[ 6.7255026e-01 -1.0000000e+09 -1.0000000e+09]\n",
      " [ 1.3069937e+00  2.5519230e+00 -1.0000000e+09]\n",
      " [ 1.8060382e-01  3.2179931e-01  1.1949346e-01]]\n",
      "\n",
      "Attention percentages:\n",
      "[[1.         0.         0.        ]\n",
      " [0.22357914 0.7764209  0.        ]\n",
      " [0.32337666 0.37241668 0.30420673]]\n",
      "\n",
      "Final attention output:\n",
      "[[0.8224545  0.52411664]\n",
      " [1.3705109  0.99325544]\n",
      " [0.95412695 0.5512294 ]]\n"
     ]
    }
   ],
   "source": [
    "# Print weights and verify calculations\n",
    "print(\"\\nQuery weights (W_q) transposed:\")\n",
    "print(masked_self_attention.W_q.value.T)\n",
    "\n",
    "print(\"\\nKey weights (W_k) transposed:\")\n",
    "print(masked_self_attention.W_k.value.T)\n",
    "\n",
    "print(\"\\nValue weights (W_v) transposed:\")\n",
    "print(masked_self_attention.W_v.value.T)\n",
    "\n",
    "# Calculate intermediate values\n",
    "q = encodings_matrix @ masked_self_attention.W_q.value\n",
    "print(\"\\nQueries (q):\")\n",
    "print(q)\n",
    "\n",
    "k = encodings_matrix @ masked_self_attention.W_k.value\n",
    "print(\"\\nKeys (k):\")\n",
    "print(k)\n",
    "\n",
    "v = encodings_matrix @ masked_self_attention.W_v.value\n",
    "print(\"\\nValues (v):\")\n",
    "print(v)\n",
    "\n",
    "sims = q @ k.swapaxes(0, 1)\n",
    "print(\"\\nSimilarity scores (sims):\")\n",
    "print(sims)\n",
    "\n",
    "scaled_sims = sims / jnp.sqrt(2)\n",
    "print(\"\\nScaled similarities:\")\n",
    "print(scaled_sims)\n",
    "\n",
    "masked_scaled_sims = jnp.where(mask, -1e9, scaled_sims)\n",
    "print(\"\\nMasked scaled similarities:\")\n",
    "print(masked_scaled_sims)\n",
    "\n",
    "attention_percents = jax.nn.softmax(masked_scaled_sims, axis=1)\n",
    "print(\"\\nAttention percentages:\")\n",
    "print(attention_percents)\n",
    "\n",
    "attention_output = attention_percents @ v\n",
    "print(\"\\nFinal attention output:\")\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
