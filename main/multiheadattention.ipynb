{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nnx.Module):\n",
    "    def __init__(self, d_model: int = 2, \n",
    "                 row_dim: int = 0, \n",
    "                 col_dim: int = 1, \n",
    "                 *, \n",
    "                 rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W_q = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.W_k = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.W_v = nnx.Param(jax.random.uniform(key, (d_model, d_model)))\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def __call__(self, encodings_for_q: jax.Array, \n",
    "                 encodings_for_k: jax.Array, \n",
    "                 encodings_for_v: jax.Array, \n",
    "                 mask: jax.Array = None):\n",
    "        q = encodings_for_q @ self.W_q\n",
    "        k = encodings_for_k @ self.W_k\n",
    "        v = encodings_for_v @ self.W_v\n",
    "\n",
    "        sims = q @ k.swapaxes(self.row_dim, self.col_dim)\n",
    "        scaled_sims = sims / jnp.sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = jnp.where(mask, -1e9, scaled_sims)\n",
    "\n",
    "        attention_percents = jax.nn.softmax(scaled_sims, axis=self.col_dim)\n",
    "        attention_scores = attention_percents @ v\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nnx.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int = 2, \n",
    "                 row_dim: int = 0, \n",
    "                 col_dim: int = 1, \n",
    "                 num_heads: int = 1, \n",
    "                 *, \n",
    "                 rngs: nnx.Rngs):\n",
    "        self.heads = [Attention(d_model, row_dim, col_dim, rngs=rngs) \n",
    "                     for _ in range(num_heads)]\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "    def __call__(self, \n",
    "                 encodings_for_q: jax.Array, \n",
    "                 encodings_for_k: jax.Array, \n",
    "                 encodings_for_v: jax.Array):\n",
    "        # Run data through all attention heads and concatenate along col_dim\n",
    "        outputs = [head(encodings_for_q, encodings_for_k, encodings_for_v) \n",
    "                  for head in self.heads]\n",
    "        return jnp.concatenate(outputs, axis=self.col_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-Decoder Attention Output:\n",
      "[[1.1276929  0.73620886]\n",
      " [1.2950552  0.9069855 ]\n",
      " [0.95412695 0.5512294 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create input matrices\n",
    "encodings_for_q = jnp.array([[1.16, 0.23],\n",
    "                           [0.57, 1.36],\n",
    "                           [4.41, -2.16]])\n",
    "encodings_for_k = jnp.array([[1.16, 0.23],\n",
    "                           [0.57, 1.36],\n",
    "                           [4.41, -2.16]])\n",
    "encodings_for_v = jnp.array([[1.16, 0.23],\n",
    "                           [0.57, 1.36],\n",
    "                           [4.41, -2.16]])\n",
    "\n",
    "# Set random seed\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Calculate Encoder-Decoder Attention\n",
    "attention = Attention(d_model=2, row_dim=0, col_dim=1, rngs=nnx.Rngs(params=key))\n",
    "attention_output = attention(encodings_for_q, encodings_for_k, encodings_for_v)\n",
    "print(\"Encoder-Decoder Attention Output:\")\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Multihead attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Head Attention (1 head) Output:\n",
      "[[1.1276929  0.73620886]\n",
      " [1.2950552  0.9069855 ]\n",
      " [0.95412695 0.5512294 ]]\n",
      "\n",
      "Multi-Head Attention (2 heads) Output:\n",
      "[[1.1276929  0.73620886 2.5678048  2.6611998 ]\n",
      " [1.2950552  0.9069855  2.4171586  2.512345  ]\n",
      " [0.95412695 0.5512294  2.815371   2.9060547 ]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Multi-Head Attention with single head\n",
    "multi_head_attention_single = MultiHeadAttention(d_model=2, \n",
    "                                               row_dim=0, \n",
    "                                               col_dim=1, \n",
    "                                               num_heads=1, \n",
    "                                               rngs=nnx.Rngs(params=key))\n",
    "single_head_output = multi_head_attention_single(encodings_for_q, \n",
    "                                               encodings_for_k, \n",
    "                                               encodings_for_v)\n",
    "print(\"\\nMulti-Head Attention (1 head) Output:\")\n",
    "print(single_head_output)\n",
    "\n",
    "# Calculate Multi-Head Attention with two heads\n",
    "multi_head_attention_double = MultiHeadAttention(d_model=2, \n",
    "                                               row_dim=0, \n",
    "                                               col_dim=1, \n",
    "                                               num_heads=2, \n",
    "                                               rngs=nnx.Rngs(params=key))\n",
    "double_head_output = multi_head_attention_double(encodings_for_q, \n",
    "                                               encodings_for_k, \n",
    "                                               encodings_for_v)\n",
    "print(\"\\nMulti-Head Attention (2 heads) Output:\")\n",
    "print(double_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
